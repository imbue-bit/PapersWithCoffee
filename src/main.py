import os
import datetime
import feedparser
import openai
from dotenv import load_dotenv
import math

load_dotenv()

API_KEY = os.environ.get("OPENAI_API_KEY")
API_BASE = os.environ.get("OPENAI_API_BASE")
MODEL_NAME = os.environ.get("OPENAI_MODEL_NAME")

RSS_FEEDS = {
    "HackerNews": "https://news.ycombinator.com/rss",
    "OpenAI": "https://openai.com/news/rss.xml",
    "arXiv cs.AI": "https://rss.arxiv.org/rss/cs.AI",
    "arXiv cs.LG": "https://rss.arxiv.org/rss/cs.LG",
}

MAX_ITEMS_PER_FEED = 75
CHUNK_SIZE = 25

def get_rss_entries():
    all_entries = []
    print("ğŸ” æ­£åœ¨ä»ä»¥ä¸‹æºè·å–æ–°é—»...")
    for name, url in RSS_FEEDS.items():
        try:
            feed = feedparser.parse(url)
            print(f"  - {name}: æˆåŠŸè·å– {len(feed.entries)} æ¡æ–°é—»")
            entries = feed.entries[:MAX_ITEMS_PER_FEED]
            for entry in entries:
                all_entries.append({
                    "source": name,
                    "title": entry.title,
                    "link": entry.link,
                    "summary": entry.get("summary", "")
                })
        except Exception as e:
            print(f"  - {name}: è·å–å¤±è´¥ï¼Œé”™è¯¯: {e}")
    return all_entries

def call_openai_api(prompt, chunk_index, total_chunks):
    if API_KEY == "YOUR_OPENAI_API_KEY_HERE" or not API_KEY:
        raise ValueError("âŒ API Key æœªè®¾ç½®ã€‚è¯·åœ¨ä»£ç ä¸­æˆ– .env æ–‡ä»¶ä¸­è¿›è¡Œé…ç½®ã€‚")

    print(f"\nğŸ§  æ­£åœ¨å¤„ç†ç¬¬ {chunk_index + 1}/{total_chunks} å—æ•°æ®ï¼Œè¿æ¥ LLM API...")
    print(f"   - APIæ¥å…¥ç‚¹: {API_BASE}")
    print(f"   - ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")

    try:
        client = openai.OpenAI(api_key=API_KEY, base_url=API_BASE)
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIé¢†åŸŸåˆ†æå¸ˆå’Œç§‘æŠ€ç¼–è¾‘ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æã€ç­›é€‰ã€æ€»ç»“å¹¶ä»¥ä¸­æ–‡æ–°é—»é£æ ¼å‘ˆç°é‡è¦çš„AIè¿›å±•ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·æŒ‡å®šçš„æ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–å†…å®¹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5,
            max_tokens=4000,
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"\nâŒ API è°ƒç”¨å¤±è´¥ (å— {chunk_index + 1}): {e}")
        return None

def build_prompt(entries_chunk):
    """
    ä¸ºå•ä¸ªæ–°é—»å—æ„å»ºPromptã€‚
    """
    formatted_entries = []
    for i, entry in enumerate(entries_chunk):
        # æ¸…ç†æ‘˜è¦ä¸­çš„HTMLæ ‡ç­¾
        summary_text = entry['summary'].replace('<p>', '').replace('</p>', '').replace('\n', ' ').strip()
        formatted_entries.append(
            f"ID:{i+1}\næ¥æº:{entry['source']}\næ ‡é¢˜:{entry['title']}\né“¾æ¥:{entry['link']}\næ‘˜è¦:{summary_text}\n---"
        )
    
    entries_text = "\n".join(formatted_entries)

    prompt = f"""
è¯·åˆ†æä»¥ä¸‹ä»å¤šä¸ªæ¥æºè·å–çš„ç§‘æŠ€æ–°é—»åˆ—è¡¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š

1.  **ç­›é€‰**ï¼šåªé€‰æ‹©ä¸äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰é¢†åŸŸç›´æ¥ç›¸å…³ï¼Œå¹¶ä¸”æ˜¯ã€Œé‡å¤§ã€æˆ–å…·æœ‰ã€Œåˆ›æ–°æ€§ã€çš„æ–°é—»ã€‚è¯·ä¸¥æ ¼è¿‡æ»¤æ‰æ— å…³ã€çç¢ã€æ— æ˜ç¡®è´¡çŒ®æˆ–é‡å¤çš„æ–°é—»ã€‚

2.  **ç”Ÿæˆè¯¦ç»†æ‘˜è¦**ï¼šå¯¹äºç­›é€‰å‡ºçš„æ¯ä¸€æ¡é‡è¦æ–°é—»ï¼š
    *   ç”¨ä¸“ä¸šä½†æ˜“äºç†è§£çš„ä¸­æ–‡ï¼Œæ’°å†™ä¸€æ®µ**è¯¦ç»†çš„æ‘˜è¦**ï¼ˆçº¦200-300å­—ï¼‰ã€‚
    *   æ‘˜è¦å¿…é¡»æ·±å…¥è§£é‡Šè¿™é¡¹å·¥ä½œçš„**æ ¸å¿ƒèƒŒæ™¯ã€å…³é”®æ–¹æ³•ã€åˆ›æ–°ç‚¹ã€ä»¥åŠæœ€ç»ˆå–å¾—çš„æˆæœæˆ–å…¶æ½œåœ¨å½±å“**ã€‚ç›®æ ‡æ˜¯è®©è¯»è€…å³ä½¿ä¸çœ‹åŸæ–‡ä¹Ÿèƒ½å……åˆ†ç†è§£å…¶ä»·å€¼ã€‚

3.  **æ’åº**ï¼šå°†ç­›é€‰å‡ºçš„æ–°é—»æŒ‰ç…§å…¶é‡è¦æ€§å’Œå½±å“åŠ›ä»é«˜åˆ°ä½è¿›è¡Œæ’åºã€‚

4.  **è¾“å‡ºæ ¼å¼ (ä¸ºæ‰“å°ä¼˜åŒ–)**ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹Markdownæ ¼å¼è¾“å‡ºã€‚**ä¸è¦åŒ…å«åŸæ–‡é“¾æ¥**ï¼Œç”¨è¯¦ç»†æ‘˜è¦ä»£æ›¿ã€‚ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–å¼€åœºç™½ã€‚

### [æ–°é—»æ ‡é¢˜]
- **æ¥æº**: [æ¥æºåç§°]
- **æ‘˜è¦**: [ä½ æ’°å†™çš„è¯¦ç»†ä¸­æ–‡æ‘˜è¦ï¼Œè¯¦ç»†è¯´æ˜èƒŒæ™¯ã€æ–¹æ³•ã€æˆæœå’Œæ„ä¹‰]

---

ä»¥ä¸‹æ˜¯å¾…å¤„ç†çš„æ–°é—»åˆ—è¡¨ï¼š

{entries_text}
"""
    return prompt

def generate_markdown_report(content):
    today_str = datetime.datetime.now().strftime("%Y-%m-%d")
    filename = f"Coffee_æ—¥æŠ¥_{today_str}.md"
    
    header = f"""# PapersWithCoffee æ—¥æŠ¥

**æ—¥æœŸ**: {today_str}
---

"""
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(header)
        f.write(content)
        
    print(f"\nâœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼æ–‡ä»¶å·²ä¿å­˜ä¸º: {filename}")

def main():
    print("="*50)
    print("ğŸ“° PapersWithCoffee æ—¥æŠ¥ ğŸ“°")
    print("="*50)

    # 1. è·å–æ‰€æœ‰æ–°é—»æ¡ç›®
    entries = get_rss_entries()
    if not entries:
        print("\nâŒ æœªèƒ½è·å–åˆ°ä»»ä½•æ–°é—»ï¼Œç¨‹åºé€€å‡ºã€‚")
        return
    
    total_entries = len(entries)
    print(f"\nğŸ“Š å…±è·å–åˆ° {total_entries} æ¡æ–°é—»æ¡ç›®ã€‚")

    # 2. åˆ†å—å¤„ç†é€»è¾‘
    num_chunks = math.ceil(total_entries / CHUNK_SIZE)
    print(f"   - å°†æ•°æ®åˆ†ä¸º {num_chunks} å—è¿›è¡Œå¤„ç† (æ¯å— {CHUNK_SIZE} æ¡)ã€‚")
    
    final_content_parts = []
    for i in range(num_chunks):
        # åˆ›å»ºå½“å‰å—çš„æ•°æ®
        start_index = i * CHUNK_SIZE
        end_index = start_index + CHUNK_SIZE
        chunk = entries[start_index:end_index]
        
        # ä¸ºå½“å‰å—æ„å»ºå¹¶è°ƒç”¨API
        prompt = build_prompt(chunk)
        processed_chunk_content = call_openai_api(prompt, i, num_chunks)
        
        if processed_chunk_content:
            final_content_parts.append(processed_chunk_content)
            print(f"   - âœ… ç¬¬ {i + 1}/{num_chunks} å—å¤„ç†æˆåŠŸã€‚")
        else:
            print(f"   - âš ï¸ ç¬¬ {i + 1}/{num_chunks} å—å¤„ç†å¤±è´¥ï¼Œè·³è¿‡æ­¤å—ã€‚")

    # 3. åˆå¹¶æ‰€æœ‰å—çš„ç»“æœå¹¶ç”ŸæˆæŠ¥å‘Š
    if final_content_parts:
        # å°†æ‰€æœ‰æˆåŠŸå¤„ç†çš„å—çš„å†…å®¹åˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²
        full_report_content = "\n".join(final_content_parts)
        print("\nğŸ“‘ æ‰€æœ‰å—å¤„ç†å®Œæ¯•ï¼Œæ­£åœ¨åˆå¹¶ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
        generate_markdown_report(full_report_content)
    else:
        print("\nâŒ æ‰€æœ‰å—å‡æœªèƒ½æˆåŠŸå¤„ç†ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚")


if __name__ == "__main__":
    main()
