import os
import asyncio
import feedparser
from openai import AsyncOpenAI
from dotenv import load_dotenv
from mcp.server import FastMCP
import math

# --- é…ç½® ---
load_dotenv()

# ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®
API_KEY = os.environ.get("OPENAI_API_KEY")
API_BASE = os.environ.get("OPENAI_API_BASE", "https://api.openai.com/v1")
MODEL_NAME = os.environ.get("OPENAI_MODEL_NAME", "gpt-4o")

# RSS Feed
RSS_FEEDS = {
    "HackerNews": "https://news.ycombinator.com/rss",
    "OpenAI": "https://openai.com/news/rss.xml",
    "arXiv cs.AI": "https://rss.arxiv.org/rss/cs.AI",
    "arXiv cs.LG": "https://rss.arxiv.org/rss/cs.LG",
}

MAX_ITEMS_PER_FEED = 75 # æ¯ä¸ªæºæœ€å¤šè·å–çš„æ–°é—»æ¡æ•°
CHUNK_SIZE = 25 # æ¯æ¬¡è¯·æ±‚ LLM å¤„ç†çš„æ–°é—»æ¡æ•°

# --- åˆå§‹åŒ– MCP åº”ç”¨ ---
app = FastMCP('ai-news-reporter')

# --- è¾…åŠ©å‡½æ•° ---
def get_rss_entries() -> list:
    """åŒæ­¥å‡½æ•°ï¼šä»æ‰€æœ‰ RSS æºè·å–æ–°é—»æ¡ç›®ã€‚"""
    all_entries = []
    print("ğŸ” æ­£åœ¨ä»ä»¥ä¸‹æºè·å–æ–°é—»...")
    for name, url in RSS_FEEDS.items():
        try:
            feed = feedparser.parse(url)
            print(f"  - {name}: æˆåŠŸè·å– {len(feed.entries)} æ¡æ–°é—»")
            entries = feed.entries[:MAX_ITEMS_PER_FEED]
            for entry in entries:
                all_entries.append({
                    "source": name,
                    "title": entry.title,
                    "link": entry.link,
                    "summary": entry.get("summary", "")
                })
        except Exception as e:
            print(f"  - {name}: è·å–å¤±è´¥ï¼Œé”™è¯¯: {e}")
    return all_entries

def build_prompt(entries_chunk: list) -> str:
    """ä¸ºå•ä¸ªæ–°é—»å—æ„å»ºPromptã€‚"""
    formatted_entries = []
    for i, entry in enumerate(entries_chunk):
        summary_text = entry['summary'].replace('<p>', '').replace('</p>', '').replace('\n', ' ').strip()
        formatted_entries.append(
            f"ID:{i+1}\næ¥æº:{entry['source']}\næ ‡é¢˜:{entry['title']}\né“¾æ¥:{entry['link']}\næ‘˜è¦:{summary_text}\n---"
        )
    entries_text = "\n".join(formatted_entries)
    return f"""
è¯·åˆ†æä»¥ä¸‹ä»å¤šä¸ªæ¥æºè·å–çš„ç§‘æŠ€æ–°é—»åˆ—è¡¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. **ç­›é€‰**ï¼šåªé€‰æ‹©ä¸äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰é¢†åŸŸç›´æ¥ç›¸å…³ï¼Œå¹¶ä¸”æ˜¯ã€Œé‡å¤§ã€æˆ–å…·æœ‰ã€Œåˆ›æ–°æ€§ã€çš„æ–°é—»ã€‚è¯·ä¸¥æ ¼è¿‡æ»¤æ‰æ— å…³ã€çç¢ã€æ— æ˜ç¡®è´¡çŒ®æˆ–é‡å¤çš„æ–°é—»ã€‚
2. **ç”Ÿæˆè¯¦ç»†æ‘˜è¦**ï¼šå¯¹äºç­›é€‰å‡ºçš„æ¯ä¸€æ¡é‡è¦æ–°é—»ï¼Œç”¨ä¸“ä¸šä½†æ˜“äºç†è§£çš„ä¸­æ–‡ï¼Œæ’°å†™ä¸€æ®µ**è¯¦ç»†çš„æ‘˜è¦**ï¼ˆçº¦200-300å­—ï¼‰ï¼Œæ·±å…¥è§£é‡Šå…¶**æ ¸å¿ƒèƒŒæ™¯ã€å…³é”®æ–¹æ³•ã€åˆ›æ–°ç‚¹ã€ä»¥åŠæœ€ç»ˆå–å¾—çš„æˆæœæˆ–å…¶æ½œåœ¨å½±å“**ã€‚
3. **æ’åº**ï¼šå°†ç­›é€‰å‡ºçš„æ–°é—»æŒ‰ç…§å…¶é‡è¦æ€§å’Œå½±å“åŠ›ä»é«˜åˆ°ä½è¿›è¡Œæ’åºã€‚
4. **è¾“å‡ºæ ¼å¼**ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹Markdownæ ¼å¼è¾“å‡ºã€‚**ä¸è¦åŒ…å«åŸæ–‡é“¾æ¥**ã€‚ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–å¼€åœºç™½ã€‚

### [æ–°é—»æ ‡é¢˜]
- **æ¥æº**: [æ¥æºåç§°]
- **æ‘˜è¦**: [ä½ æ’°å†™çš„è¯¦ç»†ä¸­æ–‡æ‘˜è¦]

---
ä»¥ä¸‹æ˜¯å¾…å¤„ç†çš„æ–°é—»åˆ—è¡¨ï¼š
{entries_text}
"""

async def call_openai_api(client: AsyncOpenAI, prompt: str, chunk_index: int, total_chunks: int) -> str | None:
    """å¼‚æ­¥è°ƒç”¨ OpenAI APIã€‚"""
    print(f"\nğŸ§  æ­£åœ¨å¤„ç†ç¬¬ {chunk_index + 1}/{total_chunks} å—æ•°æ®ï¼Œè¿æ¥ LLM API...")
    print(f"   - APIæ¥å…¥ç‚¹: {API_BASE}")
    print(f"   - ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    try:
        response = await client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIé¢†åŸŸåˆ†æå¸ˆå’Œç§‘æŠ€ç¼–è¾‘ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æã€ç­›é€‰ã€æ€»ç»“å¹¶ä»¥ä¸­æ–‡æ–°é—»é£æ ¼å‘ˆç°é‡è¦çš„AIè¿›å±•ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·æŒ‡å®šçš„æ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–å†…å®¹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5,
            max_tokens=4000,
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"\nâŒ API è°ƒç”¨å¤±è´¥ (å— {chunk_index + 1}): {e}")
        return None

@app.tool()
async def generate_ai_news_report() -> str:
    """
    è·å–æœ€æ–°çš„AIç›¸å…³RSSæ–°é—»ï¼Œé€šè¿‡å¤§æ¨¡å‹è¿›è¡Œåˆ†æã€ç­›é€‰å’Œæ€»ç»“ï¼Œå¹¶ç”Ÿæˆä¸€ä»½Markdownæ ¼å¼çš„AIæ–°é—»æ—¥æŠ¥ã€‚

    Returns:
        ä¸€ä»½åŒ…å«æœ€æ–°AIæ–°é—»æ‘˜è¦çš„Markdownæ ¼å¼å­—ç¬¦ä¸²ã€‚
    """
    # 0. æ£€æŸ¥ API Key
    if not API_KEY or API_KEY == "YOUR_OPENAI_API_KEY_HERE":
        error_msg = "âŒ API Key æœªè®¾ç½®ã€‚è¯·åœ¨æœåŠ¡ç«¯ç¯å¢ƒçš„ .env æ–‡ä»¶ä¸­è¿›è¡Œé…ç½®ã€‚"
        print(error_msg)
        return error_msg
    
    # 1. è·å–æ‰€æœ‰æ–°é—»æ¡ç›® (åœ¨å¼‚æ­¥å‡½æ•°ä¸­è¿è¡ŒåŒæ­¥ä»£ç )
    loop = asyncio.get_running_loop()
    entries = await loop.run_in_executor(None, get_rss_entries)
    if not entries:
        msg = "\nâŒ æœªèƒ½è·å–åˆ°ä»»ä½•æ–°é—»ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚"
        print(msg)
        return msg
        
    total_entries = len(entries)
    print(f"\nğŸ“Š å…±è·å–åˆ° {total_entries} æ¡æ–°é—»æ¡ç›®ã€‚")

    # 2. åˆ†å—å¤„ç†
    num_chunks = math.ceil(total_entries / CHUNK_SIZE)
    print(f"   - å°†æ•°æ®åˆ†ä¸º {num_chunks} å—è¿›è¡Œå¤„ç† (æ¯å— {CHUNK_SIZE} æ¡)ã€‚")
    
    # åˆå§‹åŒ–å¼‚æ­¥ OpenAI å®¢æˆ·ç«¯
    client = AsyncOpenAI(api_key=API_KEY, base_url=API_BASE)
    
    # å¹¶å‘å¤„ç†æ‰€æœ‰å—
    tasks = []
    for i in range(num_chunks):
        start_index = i * CHUNK_SIZE
        end_index = start_index + CHUNK_SIZE
        chunk = entries[start_index:end_index]
        prompt = build_prompt(chunk)
        tasks.append(call_openai_api(client, prompt, i, num_chunks))
    
    processed_results = await asyncio.gather(*tasks)
    
    # 3. åˆå¹¶ç»“æœå¹¶è¿”å›
    final_content_parts = [res for res in processed_results if res]
    if not final_content_parts:
        msg = "\nâŒ æ‰€æœ‰å—å‡æœªèƒ½æˆåŠŸå¤„ç†ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚"
        print(msg)
        return msg

    full_report_content = "\n".join(final_content_parts)
    print("\nâœ… æ‰€æœ‰å—å¤„ç†å®Œæ¯•ï¼ŒæŠ¥å‘Šå·²ç”Ÿæˆã€‚")
    return full_report_content

if __name__ == "__main__":
    app.run(transport='stdio')
